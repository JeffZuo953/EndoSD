[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2_LoRA",
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2_LoRA",
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2_LoRA",
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2_LoRA",
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2_LoRA",
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "ConcatDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "matplotlib.cm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.cm",
        "description": "matplotlib.cm",
        "detail": "matplotlib.cm",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt_features",
        "description": "depth_anything_v2.dpt_features",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_features",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt_features",
        "description": "depth_anything_v2.dpt_features",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_features",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "importPath": "util.metric",
        "description": "util.metric",
        "isExtraImport": true,
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "init_log",
        "importPath": "util.utils",
        "description": "util.utils",
        "isExtraImport": true,
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "isExtraImport": true,
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "isExtraImport": true,
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "isExtraImport": true,
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "to_pil_image",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "to_pil_image",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "product",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "product",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "isExtraImport": true,
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.backends.cudnn",
        "description": "torch.backends.cudnn",
        "detail": "torch.backends.cudnn",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "isExtraImport": true,
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "isExtraImport": true,
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "isExtraImport": true,
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "isExtraImport": true,
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "isExtraImport": true,
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "importPath": "util.loss",
        "description": "util.loss",
        "isExtraImport": true,
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "OpenEXR",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "OpenEXR",
        "description": "OpenEXR",
        "detail": "OpenEXR",
        "documentation": {}
    },
    {
        "label": "Imath",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "Imath",
        "description": "Imath",
        "detail": "Imath",
        "documentation": {}
    },
    {
        "label": "h5py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "h5py",
        "description": "h5py",
        "detail": "h5py",
        "documentation": {}
    },
    {
        "label": "Resize",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "Crop",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "Resize",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "Resize",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "Crop",
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "isExtraImport": true,
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "open3d",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d",
        "description": "open3d",
        "detail": "open3d",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn.parameter",
        "description": "torch.nn.parameter",
        "isExtraImport": true,
        "detail": "torch.nn.parameter",
        "documentation": {}
    },
    {
        "label": "DepthEstimatorOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "print_table",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.check_model_weights-checkpoint",
        "description": ".ipynb_checkpoints.check_model_weights-checkpoint",
        "peekOfCode": "def print_table(headers, data):\n    \"\"\"\n    n    Args:\n        headers (list): n        data (list): n    \"\"\"\n    if not data:\n        print(\"")\n        return",
        "detail": ".ipynb_checkpoints.check_model_weights-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_model_weights_info",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.check_model_weights-checkpoint",
        "description": ".ipynb_checkpoints.check_model_weights-checkpoint",
        "peekOfCode": "def get_model_weights_info(model: nn.Module):\n    \"\"\"\n     state_dict n    Args:\n        model (nn.Module): PyTorch n    Returns:\n        list: \n              (, , ?. n    \"\"\"\n    weights_info = []",
        "detail": ".ipynb_checkpoints.check_model_weights-checkpoint",
        "documentation": {}
    },
    {
        "label": "depth_to_colormap",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate-checkpoint",
        "description": ".ipynb_checkpoints.evaluate-checkpoint",
        "peekOfCode": "def depth_to_colormap(depth_map: np.ndarray) -> np.ndarray:\n    \"\"\"\n    n    Args:\n        depth_map (np.ndarray): n    Returns:\n        np.ndarray: RGB (0-255)n    \"\"\"\n    # ?-1\n    min_val = np.min(depth_map)",
        "detail": ".ipynb_checkpoints.evaluate-checkpoint",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate-checkpoint",
        "description": ".ipynb_checkpoints.evaluate-checkpoint",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace(\"module.\", \"\")\n        new_state_dict[new_key] = v\n    return new_state_dict\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": ".ipynb_checkpoints.evaluate-checkpoint",
        "documentation": {}
    },
    {
        "label": "normalize_and_save_image",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate-checkpoint",
        "description": ".ipynb_checkpoints.evaluate-checkpoint",
        "peekOfCode": "def normalize_and_save_image(data: np.ndarray, path: str, is_dino_feature: bool = False):\n    \"\"\"\n    0-255PNGn    Args:\n        data (np.ndarray): DINOn        path (str): n        is_dino_feature (bool): INOn    \"\"\"\n    if is_dino_feature and data.ndim == 3: # C, H, W\n        # For DINO features, take the first channel for visualization or average",
        "detail": ".ipynb_checkpoints.evaluate-checkpoint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate-checkpoint",
        "description": ".ipynb_checkpoints.evaluate-checkpoint",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\"{}\\n\".format(pprint.pformat(all_args)))\n    writer = SummaryWriter(args.save_path)\n    # valset = C3VD('/root/Depth-Anything-V2/metric_depth/dataset/splits/c3vd/val.txt', 'val', size=size, max_depth=args.max_depth)\n    valset = ConcatDataset([\n        # C3VD(",
        "detail": ".ipynb_checkpoints.evaluate-checkpoint",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.evaluate-checkpoint",
        "description": ".ipynb_checkpoints.evaluate-checkpoint",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\"--save-path\", type=str, required=True)\nparser.add_argument(\"--load-from\",\n                    type=str,\n                    required=True,\n                    help=\"Path to the model checkpoint\")",
        "detail": ".ipynb_checkpoints.evaluate-checkpoint",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.evaluate-checkpoint",
        "description": ".ipynb_checkpoints.evaluate-checkpoint",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\ndef main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\"{}\\n\".format(pprint.pformat(all_args)))\n    writer = SummaryWriter(args.save_path)\n    # valset = C3VD('/root/Depth-Anything-V2/metric_depth/dataset/splits/c3vd/val.txt', 'val', size=size, max_depth=args.max_depth)\n    valset = ConcatDataset([",
        "detail": ".ipynb_checkpoints.evaluate-checkpoint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate_performance-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance-checkpoint",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\n        \"Performance Test Configuration:\\n{}\\n\".format(pprint.pformat(all_args))\n    )\n    # --- Model Loading ---\n    model_configs = {",
        "detail": ".ipynb_checkpoints.evaluate_performance-checkpoint",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.evaluate_performance-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance-checkpoint",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 Metric Depth Performance Evaluation\"\n)\nparser.add_argument(\n    \"--encoder\", default=\"vitl\", choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"]\n)\nparser.add_argument(\"--img-size\", default=518, type=int)\nparser.add_argument(\n    \"--load-from\", type=str, required=True, help=\"Path to the model checkpoint\"\n)",
        "detail": ".ipynb_checkpoints.evaluate_performance-checkpoint",
        "documentation": {}
    },
    {
        "label": "preprocess_frame",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "peekOfCode": "def preprocess_frame(frame, target_size):\n    \"\"\"Converts OpenCV frame to model input tensor.\"\"\"\n    if frame is None:\n        return None\n    # 1. BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # 2. To Tensor (HWC -> CHW) and Normalize to [0, 1]\n    tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0\n    # 3. Resize\n    # Use align_corners=True consistent with many DPT models if needed, else False might be better",
        "detail": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "documentation": {}
    },
    {
        "label": "postprocess_depth",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "peekOfCode": "def postprocess_depth(pred_depth, original_hw):\n    \"\"\"Converts raw model output to a displayable depth map.\"\"\"\n    # 1. Resize to original frame size\n    # Use align_corners=True matching potential use in model, else False\n    resized_pred = F.interpolate(\n        pred_depth.unsqueeze(0).unsqueeze(0),\n        size=original_hw,\n        mode=\"bicubic\",\n        align_corners=False,\n    ).squeeze()",
        "detail": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\n        \"Real-time Inference Configuration:\\n{}\\n\".format(pprint.pformat(all_args))\n    )\n    # --- Model Loading ---\n    model_configs = {",
        "detail": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 Real-time Inference from Camera\"\n)\n# --- Model Arguments ---\nparser.add_argument(\n    \"--encoder\", default=\"vitl\", choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"]\n)\nparser.add_argument(\n    \"--load-from\", type=str, required=True, help=\"Path to the model checkpoint\"\n)",
        "detail": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "documentation": {}
    },
    {
        "label": "MEAN",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "peekOfCode": "MEAN = torch.tensor([0.485, 0.456, 0.406]).cuda().view(3, 1, 1)\nSTD = torch.tensor([0.229, 0.224, 0.225]).cuda().view(3, 1, 1)\ndef preprocess_frame(frame, target_size):\n    \"\"\"Converts OpenCV frame to model input tensor.\"\"\"\n    if frame is None:\n        return None\n    # 1. BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # 2. To Tensor (HWC -> CHW) and Normalize to [0, 1]\n    tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0",
        "detail": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "documentation": {}
    },
    {
        "label": "STD",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "description": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "peekOfCode": "STD = torch.tensor([0.229, 0.224, 0.225]).cuda().view(3, 1, 1)\ndef preprocess_frame(frame, target_size):\n    \"\"\"Converts OpenCV frame to model input tensor.\"\"\"\n    if frame is None:\n        return None\n    # 1. BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # 2. To Tensor (HWC -> CHW) and Normalize to [0, 1]\n    tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0\n    # 3. Resize",
        "detail": ".ipynb_checkpoints.evaluate_performance_with_io-checkpoint",
        "documentation": {}
    },
    {
        "label": "TemporalSmoother",
        "kind": 6,
        "importPath": ".ipynb_checkpoints.run-checkpoint",
        "description": ".ipynb_checkpoints.run-checkpoint",
        "peekOfCode": "class TemporalSmoother:\n    \"\"\"\n    Manages temporal smoothing of depth maps using multiple sliding windows,\n    one for each combination of specified initialization and scaling methods.\n    \"\"\"\n    def __init__(self,\n                 window_size: int = 30,\n                 sigma: float = 0.01,\n                 init_methods: list[str] = ['dynamic'],\n                 scale_methods: list[str] = ['least_squares'],",
        "detail": ".ipynb_checkpoints.run-checkpoint",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.run-checkpoint",
        "description": ".ipynb_checkpoints.run-checkpoint",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    \"\"\"Removes the 'module.' prefix from state dictionary keys.\"\"\"\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace('module.', '') if k.startswith('module.') else k\n        new_state_dict[new_key] = v\n    return new_state_dict\nclass TemporalSmoother:\n    \"\"\"\n    Manages temporal smoothing of depth maps using multiple sliding windows,",
        "detail": ".ipynb_checkpoints.run-checkpoint",
        "documentation": {}
    },
    {
        "label": "save_video",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.run_live-checkpoint",
        "description": ".ipynb_checkpoints.run_live-checkpoint",
        "peekOfCode": "def save_video(frames,\n               output_video_path,\n               fps=10,\n               is_depths=False,\n               grayscale=False):\n    writer = imageio.get_writer(output_video_path,\n                                fps=fps,\n                                macro_block_size=1,\n                                codec='libx264',\n                                ffmpeg_params=['-crf', '18'])",
        "detail": ".ipynb_checkpoints.run_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.run_live-checkpoint",
        "description": ".ipynb_checkpoints.run_live-checkpoint",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace(\"module.\", \"\")\n        new_state_dict[new_key] = v\n    return new_state_dict\ndef read_image_frames_from_txt(\n        filelist_path: str, image_width: int,\n        image_height: int) -> Tuple[List[np.ndarray], List[str], int]:\n    \"\"\"",
        "detail": ".ipynb_checkpoints.run_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "read_image_frames_from_txt",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.run_live-checkpoint",
        "description": ".ipynb_checkpoints.run_live-checkpoint",
        "peekOfCode": "def read_image_frames_from_txt(\n        filelist_path: str, image_width: int,\n        image_height: int) -> Tuple[List[np.ndarray], List[str], int]:\n    \"\"\"\n    Reads image paths from a text file, loads images, resizes them, and converts to numpy frames.\n    Ignores depth and mask information. Returns frames, original image paths, and a dummy fps.\n    \"\"\"\n    frames: List[np.ndarray] = []\n    image_paths: List[str] = []\n    with open(filelist_path, \"r\") as f:",
        "detail": ".ipynb_checkpoints.run_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item['image'].shape[-2])\n        max_w = max(max_w, item['image'].shape[-1])\n    stride = 14\n    if max_h % stride != 0:\n        max_h = max_h + (stride - max_h % stride)",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # PyTorch 2.0+ TF32AmpereGPUn    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "C3VD_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "C3VD_TRAIN_SPLIT = \"/data/c3vd/train.txt\"\nC3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "C3VD_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "C3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "ENDOMAPER_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "ENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "INHOUSE_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "INHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "SIMCOL_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "SIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "SIMCOL_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "SIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "description": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],\n)",
        "detail": ".ipynb_checkpoints.train_4_diff-checkpoint",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item['image'].shape[-2])\n        max_w = max(max_w, item['image'].shape[-1])\n    stride = 14\n    if max_h % stride != 0:\n        max_h = max_h + (stride - max_h % stride)",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # PyTorch 2.0+ TF32AmpereGPUn    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "C3VD_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "C3VD_TRAIN_SPLIT = \"/data/c3vd/train.txt\"\nC3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "C3VD_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "C3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "ENDOMAPER_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "ENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "INHOUSE_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "INHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "SIMCOL_TRAIN_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "SIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "SIMCOL_VAL_SPLIT",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "SIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_4_same-checkpoint",
        "description": ".ipynb_checkpoints.train_4_same-checkpoint",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],\n)",
        "detail": ".ipynb_checkpoints.train_4_same-checkpoint",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.train_single-checkpoint",
        "description": ".ipynb_checkpoints.train_single-checkpoint",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    warnings.simplefilter('ignore', np.RankWarning)\n    logger = init_log('global', logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:\n        all_args = {**vars(args), 'ngpus': world_size}\n        logger.info('{}\\n'.format(pprint.pformat(all_args)))\n        writer = SummaryWriter(args.save_path)",
        "detail": ".ipynb_checkpoints.train_single-checkpoint",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": ".ipynb_checkpoints.train_single-checkpoint",
        "description": ".ipynb_checkpoints.train_single-checkpoint",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Depth Anything V2 for Metric Depth Estimation')\nparser.add_argument('--encoder', default='vitl', choices=['vits', 'vitb', 'vitl', 'vitg'])\nparser.add_argument('--dataset', default='hypersim', choices=['hypersim', 'vkitti'])\nparser.add_argument('--img-size', default=518, type=int)\nparser.add_argument('--min-depth', default=0.001, type=float)\nparser.add_argument('--max-depth', default=20, type=float)\nparser.add_argument('--epochs', default=40, type=int)\nparser.add_argument('--bs', default=2, type=int)\nparser.add_argument('--lr', default=0.000005, type=float)\nparser.add_argument('--pretrained-from', type=str)",
        "detail": ".ipynb_checkpoints.train_single-checkpoint",
        "documentation": {}
    },
    {
        "label": "print_keys_with_depth",
        "kind": 2,
        "importPath": ".ipynb_checkpoints.weight-checkpoint",
        "description": ".ipynb_checkpoints.weight-checkpoint",
        "peekOfCode": "def print_keys_with_depth(data, current_depth, max_depth):\n    \"\"\"\n    Recursively prints keys of nested dictionaries or elements up to a specified depth.\n    Args:\n        data (dict or any): The data structure to traverse.\n        current_depth (int): The current depth of the traversal.\n        max_depth (int): The maximum depth to print keys.\n    \"\"\"\n    if current_depth > max_depth:\n        return",
        "detail": ".ipynb_checkpoints.weight-checkpoint",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "kind": 6,
        "importPath": "dataset..ipynb_checkpoints.c3vd-checkpoint",
        "description": "dataset..ipynb_checkpoints.c3vd-checkpoint",
        "peekOfCode": "class C3VD(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.1):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, 'r') as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([\n            Resize(",
        "detail": "dataset..ipynb_checkpoints.c3vd-checkpoint",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "kind": 6,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "class Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([\n            Resize(",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "FAR",
        "kind": 5,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "FAR = 4.0\nNEAR = 0.01\n# \n_X = 1.0 - FAR / NEAR\n_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "NEAR",
        "kind": 5,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "NEAR = 0.01\n# \n_X = 1.0 - FAR / NEAR\n_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "_X",
        "kind": 5,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "_X = 1.0 - FAR / NEAR\n_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "_Y",
        "kind": 5,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "Z",
        "kind": 5,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "Z = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "description": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "peekOfCode": "W = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([",
        "detail": "dataset..ipynb_checkpoints.endomapper-checkpoint",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "kind": 6,
        "importPath": "dataset..ipynb_checkpoints.inhouse-checkpoint",
        "description": "dataset..ipynb_checkpoints.inhouse-checkpoint",
        "peekOfCode": "class InHouse(Dataset):\n    def __init__(self,\n                 filelist_path: str,\n                 mode: str,\n                 size: Tuple[int, int] = (960, 540),\n                 max_depth: float = 0.05):\n        self.mode: str = mode\n        self.size: Tuple[int, int] = size\n        self.max_depth: float = max_depth\n        with open(filelist_path, \"r\") as f:",
        "detail": "dataset..ipynb_checkpoints.inhouse-checkpoint",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "kind": 6,
        "importPath": "dataset..ipynb_checkpoints.simcol-checkpoint",
        "description": "dataset..ipynb_checkpoints.simcol-checkpoint",
        "peekOfCode": "class Simcol(Dataset):\n    def __init__(self, filelist_path, mode, size=(475, 475), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        # \n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([",
        "detail": "dataset..ipynb_checkpoints.simcol-checkpoint",
        "documentation": {}
    },
    {
        "label": "C3VD",
        "kind": 6,
        "importPath": "dataset.c3vd",
        "description": "dataset.c3vd",
        "peekOfCode": "class C3VD(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.1):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, 'r') as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([\n            Resize(",
        "detail": "dataset.c3vd",
        "documentation": {}
    },
    {
        "label": "Endomapper",
        "kind": 6,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "class Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([\n            Resize(",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "FAR",
        "kind": 5,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "FAR = 4.0\nNEAR = 0.01\n# \n_X = 1.0 - FAR / NEAR\n_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "NEAR",
        "kind": 5,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "NEAR = 0.01\n# \n_X = 1.0 - FAR / NEAR\n_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "_X",
        "kind": 5,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "_X = 1.0 - FAR / NEAR\n_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "_Y",
        "kind": 5,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "_Y = FAR / NEAR\nZ = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Z",
        "kind": 5,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "Z = _X / FAR\nW = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "dataset.endomapper",
        "description": "dataset.endomapper",
        "peekOfCode": "W = _Y / FAR\nclass Endomapper(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([",
        "detail": "dataset.endomapper",
        "documentation": {}
    },
    {
        "label": "Hypersim",
        "kind": 6,
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "peekOfCode": "class Hypersim(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518)):\n        self.mode = mode\n        self.size = size\n        with open(filelist_path, 'r') as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([\n            Resize(\n                width=net_w,",
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "hypersim_distance_to_depth",
        "kind": 2,
        "importPath": "dataset.hypersim",
        "description": "dataset.hypersim",
        "peekOfCode": "def hypersim_distance_to_depth(npyDistance):\n    intWidth, intHeight, fltFocal = 1024, 768, 886.81\n    npyImageplaneX = np.linspace((-0.5 * intWidth) + 0.5, (0.5 * intWidth) - 0.5, intWidth).reshape(\n        1, intWidth).repeat(intHeight, 0).astype(np.float32)[:, :, None]\n    npyImageplaneY = np.linspace((-0.5 * intHeight) + 0.5, (0.5 * intHeight) - 0.5,\n                                 intHeight).reshape(intHeight, 1).repeat(intWidth, 1).astype(np.float32)[:, :, None]\n    npyImageplaneZ = np.full([intHeight, intWidth, 1], fltFocal, np.float32)\n    npyImageplane = np.concatenate(\n        [npyImageplaneX, npyImageplaneY, npyImageplaneZ], 2)\n    npyDepth = npyDistance / np.linalg.norm(npyImageplane, 2, 2) * fltFocal",
        "detail": "dataset.hypersim",
        "documentation": {}
    },
    {
        "label": "InHouse",
        "kind": 6,
        "importPath": "dataset.inhouse",
        "description": "dataset.inhouse",
        "peekOfCode": "class InHouse(Dataset):\n    def __init__(self,\n                 filelist_path: str,\n                 mode: str,\n                 size: Tuple[int, int] = (960, 540),\n                 max_depth: float = 0.05):\n        self.mode: str = mode\n        self.size: Tuple[int, int] = size\n        self.max_depth: float = max_depth\n        with open(filelist_path, \"r\") as f:",
        "detail": "dataset.inhouse",
        "documentation": {}
    },
    {
        "label": "KITTI",
        "kind": 6,
        "importPath": "dataset.kitti",
        "description": "dataset.kitti",
        "peekOfCode": "class KITTI(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518)):\n        if mode != 'val':\n            raise NotImplementedError\n        self.mode = mode\n        self.size = size\n        with open(filelist_path, 'r') as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([",
        "detail": "dataset.kitti",
        "documentation": {}
    },
    {
        "label": "collect_rgb_depth_pairs",
        "kind": 2,
        "importPath": "dataset.map_data",
        "description": "dataset.map_data",
        "peekOfCode": "def collect_rgb_depth_pairs(root_dir, output_txt, rgb_ext=\".png\", depth_ext=\".exr\"):\n    pairs = []\n    for seq_name in sorted(os.listdir(root_dir)):\n        seq_path = os.path.join(root_dir, seq_name)\n        if not os.path.isdir(seq_path):\n            continue\n        rgb_dir = os.path.join(seq_path, 'rgb')\n        depth_dir = os.path.join(seq_path, 'depth')\n        if not os.path.isdir(rgb_dir) or not os.path.isdir(depth_dir):\n            continue",
        "detail": "dataset.map_data",
        "documentation": {}
    },
    {
        "label": "Simcol",
        "kind": 6,
        "importPath": "dataset.simcol",
        "description": "dataset.simcol",
        "peekOfCode": "class Simcol(Dataset):\n    def __init__(self, filelist_path, mode, size=(475, 475), max_depth=0.2):\n        self.mode = mode\n        self.size = size\n        self.max_depth = max_depth\n        # \n        with open(filelist_path, \"r\") as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([",
        "detail": "dataset.simcol",
        "documentation": {}
    },
    {
        "label": "Resize",
        "kind": 6,
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "peekOfCode": "class Resize(object):\n    \"\"\"Resize sample to given size (width, height).\n    \"\"\"\n    def __init__(\n        self,\n        width,\n        height,\n        resize_target=True,\n        keep_aspect_ratio=False,\n        ensure_multiple_of=1,",
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "kind": 6,
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "peekOfCode": "class NormalizeImage(object):\n    \"\"\"Normlize image by given mean and std.\n    \"\"\"\n    def __init__(self, mean, std):\n        self.__mean = mean\n        self.__std = std\n    def __call__(self, sample):\n        sample[\"image\"] = (sample[\"image\"] - self.__mean) / self.__std\n        return sample\nclass PrepareForNet(object):",
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "kind": 6,
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "peekOfCode": "class PrepareForNet(object):\n    \"\"\"Prepare sample for usage as network input.\n    \"\"\"\n    def __init__(self):\n        pass\n    def __call__(self, sample):\n        image = np.transpose(sample[\"image\"], (2, 0, 1))\n        sample[\"image\"] = np.ascontiguousarray(image).astype(np.float32)\n        if \"mask\" in sample:\n            sample[\"mask\"] = sample[\"mask\"].astype(np.float32)",
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "Crop",
        "kind": 6,
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "peekOfCode": "class Crop(object):\n    \"\"\"Crop sample for batch-wise training. Image is of shape CxHxW\n    \"\"\"\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            self.size = size\n    def __call__(self, sample):\n        h, w = sample['image'].shape[-2:]",
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "apply_min_size",
        "kind": 2,
        "importPath": "dataset.transform",
        "description": "dataset.transform",
        "peekOfCode": "def apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"Rezise the sample to ensure the given size. Keeps aspect ratio.\n    Args:\n        sample (dict): sample\n        size (tuple): image size\n    Returns:\n        tuple: new size\n    \"\"\"\n    shape = list(sample[\"disparity\"].shape)\n    if shape[0] >= size[0] and shape[1] >= size[1]:",
        "detail": "dataset.transform",
        "documentation": {}
    },
    {
        "label": "VKITTI2",
        "kind": 6,
        "importPath": "dataset.vkitti2",
        "description": "dataset.vkitti2",
        "peekOfCode": "class VKITTI2(Dataset):\n    def __init__(self, filelist_path, mode, size=(518, 518)):\n        self.mode = mode\n        self.size = size\n        with open(filelist_path, 'r') as f:\n            self.filelist = f.read().splitlines()\n        net_w, net_h = size\n        self.transform = Compose([\n            Resize(\n                width=net_w,",
        "detail": "dataset.vkitti2",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature, out_feature, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_feature),\n            nn.ReLU(True)\n        )\n    def forward(self, x):\n        return self.conv_block(x)",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(\n        self, \n        in_channels, \n        features=256, \n        use_bn=False, \n        out_channels=[256, 512, 1024, 1024], \n        use_clstoken=False\n    ):\n        super(DPTHead, self).__init__()",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "peekOfCode": "class DepthAnythingV2(nn.Module):\n    def __init__(\n        self, \n        encoder='vitl', \n        features=256, \n        out_channels=[256, 512, 1024, 1024], \n        use_bn=False, \n        use_clstoken=False,\n        max_depth=20.0\n    ):",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt-checkpoint",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature, out_feature, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_feature),\n            nn.ReLU(True)\n        )\n    def forward(self, x):\n        return self.conv_block(x)",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(\n        self, \n        in_channels, \n        features=256, \n        use_bn=False, \n        out_channels=[256, 512, 1024, 1024], \n        use_clstoken=False\n    ):\n        super(DPTHead, self).__init__()",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "peekOfCode": "class DepthAnythingV2(nn.Module):\n    def __init__(\n        self, \n        encoder='vitl', \n        features=256, \n        out_channels=[256, 512, 1024, 1024], \n        use_bn=False, \n        use_clstoken=False,\n        max_depth=20.0\n    ):",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_features-checkpoint",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature,\n                      out_feature,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1), nn.BatchNorm2d(out_feature), nn.ReLU(True))\n    def forward(self, x):",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 features=256,\n                 use_bn=False,\n                 out_channels=[256, 512, 1024, 1024],\n                 use_clstoken=False):\n        super(DPTHead, self).__init__()\n        self.use_clstoken = use_clstoken\n        self.projects = nn.ModuleList([",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "kind": 6,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "class DepthAnythingV2(nn.Module):\n    def __init__(self,\n                 encoder='vitl',\n                 features=256,\n                 out_channels=[256, 512, 1024, 1024],\n                 use_bn=False,\n                 use_clstoken=False,\n                 max_depth=20.0):\n        super(DepthAnythingV2, self).__init__()\n        self.intermediate_layer_idx = {",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_interpolate_frames",
        "kind": 2,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "def get_interpolate_frames(frame_list_pre, frame_list_post):\n    # # Debugging: Check if frame_list_pre and frame_list_post are identical\n    # is_identical = True\n    # for idx in range(len(frame_list_pre)):\n    #     if not np.allclose(frame_list_pre[idx], frame_list_post[idx]):\n    #         is_identical = False\n    #         break\n    # print(f\"Are frame_list_pre and frame_list_post identical? {is_identical}\")\n    assert len(frame_list_pre) == len(frame_list_post)\n    min_w = 0.0",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "INFER_LEN",
        "kind": 5,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "INFER_LEN = 32\nOVERLAP = 10\nKEYFRAMES = [0, 12, 24, 25, 26, 27, 28, 29, 30, 31]\nINTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "OVERLAP",
        "kind": 5,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "OVERLAP = 10\nKEYFRAMES = [0, 12, 24, 25, 26, 27, 28, 29, 30, 31]\nINTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "KEYFRAMES",
        "kind": 5,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "KEYFRAMES = [0, 12, 24, 25, 26, 27, 28, 29, 30, 31]\nINTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "INTERP_LEN",
        "kind": 5,
        "importPath": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "description": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "peekOfCode": "INTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n        size=size,",
        "detail": "depth_anything_v2..ipynb_checkpoints.dpt_live-checkpoint",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.attention",
        "description": "depth_anything_v2.dinov2_layers.attention",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -> None:",
        "detail": "depth_anything_v2.dinov2_layers.attention",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.attention",
        "description": "depth_anything_v2.dinov2_layers.attention",
        "peekOfCode": "class MemEffAttention(Attention):\n    def forward(self, x: Tensor, attn_bias=None) -> Tensor:\n        if not XFORMERS_AVAILABLE:\n            assert attn_bias is None, \"xFormers is required for nested tensors usage\"\n            return super().forward(x)\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        q, k, v = unbind(qkv, 2)\n        x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)\n        x = x.reshape([B, N, C])",
        "detail": "depth_anything_v2.dinov2_layers.attention",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "depth_anything_v2.dinov2_layers.attention",
        "description": "depth_anything_v2.dinov2_layers.attention",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ntry:\n    from xformers.ops import memory_efficient_attention, unbind, fmha\n    XFORMERS_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"xFormers not available\")\n    XFORMERS_AVAILABLE = False\nclass Attention(nn.Module):\n    def __init__(\n        self,",
        "detail": "depth_anything_v2.dinov2_layers.attention",
        "documentation": {}
    },
    {
        "label": "Attention_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.attention_lora",
        "description": "depth_anything_v2.dinov2_layers.attention_lora",
        "peekOfCode": "class Attention_LoRA(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        # --- LoRA Args --- #",
        "detail": "depth_anything_v2.dinov2_layers.attention_lora",
        "documentation": {}
    },
    {
        "label": "MemEffAttention_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.attention_lora",
        "description": "depth_anything_v2.dinov2_layers.attention_lora",
        "peekOfCode": "class MemEffAttention_LoRA(Attention_LoRA):\n    # Inherits modified __init__ from Attention_LoRA\n    def forward(self, x: Tensor, attn_bias=None) -> Tensor:\n        if not XFORMERS_AVAILABLE:\n            assert attn_bias is None, \"xFormers is required for nested tensors usage with LoRA attention\"\n            # Fall back to parent LoRA implementation\n            return super().forward(x)\n        B, N, C = x.shape\n        # self.qkv is already LoRALinear\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)",
        "detail": "depth_anything_v2.dinov2_layers.attention_lora",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "depth_anything_v2.dinov2_layers.attention_lora",
        "description": "depth_anything_v2.dinov2_layers.attention_lora",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ntry:\n    from xformers.ops import memory_efficient_attention, unbind\n    XFORMERS_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"xFormers not available for LoRA attention\")\n    XFORMERS_AVAILABLE = False\n# Renamed to Attention_LoRA\nclass Attention_LoRA(nn.Module):\n    def __init__(",
        "detail": "depth_anything_v2.dinov2_layers.attention_lora",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        ffn_bias: bool = True,\n        drop: float = 0.0,",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "NestedTensorBlock",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "class NestedTensorBlock(Block):\n    def forward_nested(self, x_list: List[Tensor]) -> List[Tensor]:\n        \"\"\"\n        x_list contains a list of tensors to nest together and run\n        \"\"\"\n        assert isinstance(self.attn, MemEffAttention)\n        if self.training and self.sample_drop_ratio > 0.0:\n            def attn_residual_func(x: Tensor, attn_bias=None) -> Tensor:\n                return self.attn(self.norm1(x), attn_bias=attn_bias)\n            def ffn_residual_func(x: Tensor, attn_bias=None) -> Tensor:",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "def drop_add_residual_stochastic_depth(\n    x: Tensor,\n    residual_func: Callable[[Tensor], Tensor],\n    sample_drop_ratio: float = 0.0,\n) -> Tensor:\n    # 1) extract subset using permutation\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    x_subset = x[brange]",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "get_branges_scales",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "def get_branges_scales(x, sample_drop_ratio=0.0):\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    residual_scale_factor = b / sample_subset_size\n    return brange, residual_scale_factor\ndef add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "add_residual",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "def add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)\n        x_plus_residual = torch.index_add(x_flat, 0, brange, residual.to(dtype=x.dtype), alpha=residual_scale_factor)\n    else:\n        x_plus_residual = scaled_index_add(\n            x, brange, residual.to(dtype=x.dtype), scaling=scaling_vector, alpha=residual_scale_factor\n        )\n    return x_plus_residual",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "get_attn_bias_and_cat",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "def get_attn_bias_and_cat(x_list, branges=None):\n    \"\"\"\n    this will perform the index select, cat the tensors, and provide the attn_bias from cache\n    \"\"\"\n    batch_sizes = [b.shape[0] for b in branges] if branges is not None else [x.shape[0] for x in x_list]\n    all_shapes = tuple((b, x.shape[1]) for b, x in zip(batch_sizes, x_list))\n    if all_shapes not in attn_bias_cache.keys():\n        seqlens = []\n        for b, x in zip(batch_sizes, x_list):\n            for _ in range(b):",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth_list",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "def drop_add_residual_stochastic_depth_list(\n    x_list: List[Tensor],\n    residual_func: Callable[[Tensor, Any], Tensor],\n    sample_drop_ratio: float = 0.0,\n    scaling_vector=None,\n) -> Tensor:\n    # 1) generate random set of indices for dropping samples in the batch\n    branges_scales = [get_branges_scales(x, sample_drop_ratio=sample_drop_ratio) for x in x_list]\n    branges = [s[0] for s in branges_scales]\n    residual_scale_factors = [s[1] for s in branges_scales]",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "depth_anything_v2.dinov2_layers.block",
        "description": "depth_anything_v2.dinov2_layers.block",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ntry:\n    from xformers.ops import fmha\n    from xformers.ops import scaled_index_add, index_select_cat\n    XFORMERS_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"xFormers not available\")\n    XFORMERS_AVAILABLE = False\nclass Block(nn.Module):\n    def __init__(",
        "detail": "depth_anything_v2.dinov2_layers.block",
        "documentation": {}
    },
    {
        "label": "Block_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "class Block_LoRA(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        ffn_bias: bool = True,\n        drop: float = 0.0,",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "NestedTensorBlock_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "class NestedTensorBlock_LoRA(Block_LoRA):\n    # Inherits LoRA-modified __init__\n    def forward_nested(self, x_list: List[Tensor]) -> List[Tensor]:\n        assert isinstance(self.attn, MemEffAttention_LoRA), \"NestedTensor requires MemEffAttention_LoRA\"\n        assert XFORMERS_AVAILABLE, \"Please install xFormers for nested tensors usage (LoRA version)\"\n        if self.training and self.sample_drop_ratio > 0.0:\n            def attn_residual_func(x: Tensor, attn_bias=None) -> Tensor:\n                # Pass attn_bias to LoRA attention layer\n                return self.attn(self.norm1(x), attn_bias=attn_bias)\n            def ffn_residual_func(x: Tensor, attn_bias=None) -> Tensor:",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "def drop_add_residual_stochastic_depth(\n    x: Tensor,\n    residual_func: Callable[[Tensor], Tensor],\n    sample_drop_ratio: float = 0.0,\n) -> Tensor:\n    # 1) extract subset using permutation\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    x_subset = x[brange]",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "get_branges_scales",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "def get_branges_scales(x, sample_drop_ratio=0.0):\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    residual_scale_factor = b / sample_subset_size\n    return brange, residual_scale_factor\ndef add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "add_residual",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "def add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)\n        x_plus_residual = torch.index_add(x_flat, 0, brange, residual.to(dtype=x.dtype), alpha=residual_scale_factor)\n    else:\n        x_plus_residual = scaled_index_add(\n            x, brange, residual.to(dtype=x.dtype), scaling=scaling_vector, alpha=residual_scale_factor\n        )\n    return x_plus_residual",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "get_attn_bias_and_cat",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "def get_attn_bias_and_cat(x_list, branges=None):\n    batch_sizes = [b.shape[0] for b in branges] if branges is not None else [x.shape[0] for x in x_list]\n    all_shapes = tuple((b, x.shape[1]) for b, x in zip(batch_sizes, x_list))\n    cache_key = (*all_shapes, x_list[0].dtype, x_list[0].device)\n    if cache_key not in attn_bias_cache.keys():\n        seqlens = []\n        for b, x in zip(batch_sizes, x_list):\n            for _ in range(b):\n                seqlens.append(x.shape[1])\n        attn_bias = fmha.BlockDiagonalMask.from_seqlens(seqlens, B=len(seqlens))",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth_list",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "def drop_add_residual_stochastic_depth_list(\n    x_list: List[Tensor],\n    residual_func: Callable[[Tensor, Any], Tensor],\n    sample_drop_ratio: float = 0.0,\n    scaling_vector=None,\n) -> List[Tensor]:\n    branges_scales = [get_branges_scales(x, sample_drop_ratio=sample_drop_ratio) for x in x_list]\n    branges = [s[0] for s in branges_scales]\n    residual_scale_factors = [s[1] for s in branges_scales]\n    attn_bias, x_cat = get_attn_bias_and_cat(x_list, branges)",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "depth_anything_v2.dinov2_layers.block_lora",
        "description": "depth_anything_v2.dinov2_layers.block_lora",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ntry:\n    from xformers.ops import fmha\n    from xformers.ops import scaled_index_add, index_select_cat\n    XFORMERS_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"xFormers not available for LoRA block\")\n    XFORMERS_AVAILABLE = False\n# Original utility functions (drop_add_residual_stochastic_depth*, get_branges_scales, add_residual, etc.)\n# are assumed to be compatible or need slight adjustments if they directly access weights.",
        "detail": "depth_anything_v2.dinov2_layers.block_lora",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.drop_path",
        "description": "depth_anything_v2.dinov2_layers.drop_path",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "depth_anything_v2.dinov2_layers.drop_path",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.drop_path",
        "description": "depth_anything_v2.dinov2_layers.drop_path",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0:\n        random_tensor.div_(keep_prob)\n    output = x * random_tensor\n    return output",
        "detail": "depth_anything_v2.dinov2_layers.drop_path",
        "documentation": {}
    },
    {
        "label": "LayerScale",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.layer_scale",
        "description": "depth_anything_v2.dinov2_layers.layer_scale",
        "peekOfCode": "class LayerScale(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        init_values: Union[float, Tensor] = 1e-5,\n        inplace: bool = False,\n    ) -> None:\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))",
        "detail": "depth_anything_v2.dinov2_layers.layer_scale",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.lora",
        "description": "depth_anything_v2.dinov2_layers.lora",
        "peekOfCode": "class LoRALinear(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False, # Set True if the layer is ViT's c_proj\n        merge_weights: bool = True,",
        "detail": "depth_anything_v2.dinov2_layers.lora",
        "documentation": {}
    },
    {
        "label": "mark_only_lora_as_trainable",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.lora",
        "description": "depth_anything_v2.dinov2_layers.lora",
        "peekOfCode": "def mark_only_lora_as_trainable(model: nn.Module, bias: str = 'none') -> None:\n    \"\"\"\n    Freezes all parameters except LoRA parameters.\n    Args:\n        model: The model to modify.\n        bias: Strategy for handling bias parameters ('none', 'lora_only', 'all').\n              'none': All bias parameters are frozen.\n              'lora_only': Only bias parameters in LoRALinear layers are trainable.\n              'all': All bias parameters are trainable. (Not typically used with LoRA)\n    \"\"\"",
        "detail": "depth_anything_v2.dinov2_layers.lora",
        "documentation": {}
    },
    {
        "label": "lora_state_dict",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.lora",
        "description": "depth_anything_v2.dinov2_layers.lora",
        "peekOfCode": "def lora_state_dict(model: nn.Module, bias: str = 'none') -> dict:\n    \"\"\"\n    Returns a state dict containing only the trainable LoRA parameters (and potentially biases).\n    \"\"\"\n    my_state_dict = model.state_dict()\n    if bias == 'none':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k}\n    elif bias == 'lora_only':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k or ('bias' in k and isinstance(model.get_submodule(k.rsplit('.', 1)[0]), LoRALinear))}\n    elif bias == 'all':",
        "detail": "depth_anything_v2.dinov2_layers.lora",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.mlp",
        "description": "depth_anything_v2.dinov2_layers.mlp",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "depth_anything_v2.dinov2_layers.mlp",
        "documentation": {}
    },
    {
        "label": "Mlp_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.mlp_lora",
        "description": "depth_anything_v2.dinov2_layers.mlp_lora",
        "peekOfCode": "class Mlp_LoRA(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n        # --- LoRA Args --- #",
        "detail": "depth_anything_v2.dinov2_layers.mlp_lora",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.patch_embed",
        "description": "depth_anything_v2.dinov2_layers.patch_embed",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n    Args:\n        img_size: Image size.\n        patch_size: Patch token size.\n        in_chans: Number of input image channels.\n        embed_dim: Number of linear projection output channels.\n        norm_layer: Normalization layer.\n    \"\"\"",
        "detail": "depth_anything_v2.dinov2_layers.patch_embed",
        "documentation": {}
    },
    {
        "label": "make_2tuple",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_layers.patch_embed",
        "description": "depth_anything_v2.dinov2_layers.patch_embed",
        "peekOfCode": "def make_2tuple(x):\n    if isinstance(x, tuple):\n        assert len(x) == 2\n        return x\n    assert isinstance(x, int)\n    return (x, x)\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n    Args:",
        "detail": "depth_anything_v2.dinov2_layers.patch_embed",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFN",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.swiglu_ffn",
        "description": "depth_anything_v2.dinov2_layers.swiglu_ffn",
        "peekOfCode": "class SwiGLUFFN(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "depth_anything_v2.dinov2_layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFNFused",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.swiglu_ffn",
        "description": "depth_anything_v2.dinov2_layers.swiglu_ffn",
        "peekOfCode": "class SwiGLUFFNFused(SwiGLU):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "depth_anything_v2.dinov2_layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFN_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.swiglu_ffn_lora",
        "description": "depth_anything_v2.dinov2_layers.swiglu_ffn_lora",
        "peekOfCode": "class SwiGLUFFN_LoRA(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.SiLU,\n        drop: float = 0.0,\n        bias: bool = True,\n        # --- LoRA Args --- #",
        "detail": "depth_anything_v2.dinov2_layers.swiglu_ffn_lora",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFNFused_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_layers.swiglu_ffn_lora",
        "description": "depth_anything_v2.dinov2_layers.swiglu_ffn_lora",
        "peekOfCode": "class SwiGLUFFNFused_LoRA(SwiGLUFFN_LoRA):\n    # Inherits the modified __init__ from SwiGLUFFN_LoRA\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.SiLU,\n        drop: float = 0.0,\n        bias: bool = True,",
        "detail": "depth_anything_v2.dinov2_layers.swiglu_ffn_lora",
        "documentation": {}
    },
    {
        "label": "ResidualConvUnit",
        "kind": 6,
        "importPath": "depth_anything_v2.util.blocks",
        "description": "depth_anything_v2.util.blocks",
        "peekOfCode": "class ResidualConvUnit(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n    def __init__(self, features, activation, bn):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n        self.bn = bn",
        "detail": "depth_anything_v2.util.blocks",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "depth_anything_v2.util.blocks",
        "description": "depth_anything_v2.util.blocks",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n    def __init__(\n        self, \n        features, \n        activation, \n        deconv=False, \n        bn=False, \n        expand=False, ",
        "detail": "depth_anything_v2.util.blocks",
        "documentation": {}
    },
    {
        "label": "Resize",
        "kind": 6,
        "importPath": "depth_anything_v2.util.transform",
        "description": "depth_anything_v2.util.transform",
        "peekOfCode": "class Resize(object):\n    \"\"\"Resize sample to given size (width, height).\n    \"\"\"\n    def __init__(\n        self,\n        width,\n        height,\n        resize_target=True,\n        keep_aspect_ratio=False,\n        ensure_multiple_of=1,",
        "detail": "depth_anything_v2.util.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "kind": 6,
        "importPath": "depth_anything_v2.util.transform",
        "description": "depth_anything_v2.util.transform",
        "peekOfCode": "class NormalizeImage(object):\n    \"\"\"Normlize image by given mean and std.\n    \"\"\"\n    def __init__(self, mean, std):\n        self.__mean = mean\n        self.__std = std\n    def __call__(self, sample):\n        sample[\"image\"] = (sample[\"image\"] - self.__mean) / self.__std\n        return sample\nclass PrepareForNet(object):",
        "detail": "depth_anything_v2.util.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "kind": 6,
        "importPath": "depth_anything_v2.util.transform",
        "description": "depth_anything_v2.util.transform",
        "peekOfCode": "class PrepareForNet(object):\n    \"\"\"Prepare sample for usage as network input.\n    \"\"\"\n    def __init__(self):\n        pass\n    def __call__(self, sample):\n        image = np.transpose(sample[\"image\"], (2, 0, 1))\n        sample[\"image\"] = np.ascontiguousarray(image).astype(np.float32)\n        if \"depth\" in sample:\n            depth = sample[\"depth\"].astype(np.float32)",
        "detail": "depth_anything_v2.util.transform",
        "documentation": {}
    },
    {
        "label": "BlockChunk",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "class BlockChunk(nn.ModuleList):\n    def forward(self, x):\n        for b in self:\n            x = b(x)\n        return x\nclass DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "class DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "named_apply",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module\nclass BlockChunk(nn.ModuleList):",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "init_weights_vit_timm",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n    \"\"\"ViT weight initialization, original timm impl (for reproducibility)\"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\ndef vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "vit_small",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "vit_base",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def vit_base(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "vit_large",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def vit_large(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "vit_giant2",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def vit_giant2(patch_size=16, num_register_tokens=0, **kwargs):\n    \"\"\"\n    Close to ViT-giant, with embed-dim 1536 and 24 heads => embed-dim per head 64\n    \"\"\"\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1536,\n        depth=40,\n        num_heads=24,\n        mlp_ratio=4,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "DINOv2",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "def DINOv2(model_name):\n    model_zoo = {\n        \"vits\": vit_small, \n        \"vitb\": vit_base, \n        \"vitl\": vit_large, \n        \"vitg\": vit_giant2\n    }\n    return model_zoo[model_name](\n        img_size=518,\n        patch_size=14,",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "depth_anything_v2.dinov2",
        "description": "depth_anything_v2.dinov2",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module",
        "detail": "depth_anything_v2.dinov2",
        "documentation": {}
    },
    {
        "label": "BlockChunk",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "class BlockChunk(nn.ModuleList):\n    def forward(self, x):\n        for b in self:\n            x = b(x)\n        return x\nclass DinoVisionTransformer_LoRA(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "class DinoVisionTransformer_LoRA(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "named_apply",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module\nclass BlockChunk(nn.ModuleList):",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "init_weights_vit_timm",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n    \"\"\"ViT weight initialization, original timm impl (for reproducibility)\"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\ndef vit_small_lora(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer_LoRA(\n        patch_size=patch_size,\n        embed_dim=384,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "vit_small_lora",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def vit_small_lora(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer_LoRA(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        block_fn=partial(Block_LoRA, attn_class=MemEffAttention_LoRA),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "vit_base_lora",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def vit_base_lora(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer_LoRA(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        block_fn=partial(Block_LoRA, attn_class=MemEffAttention_LoRA),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "vit_large_lora",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def vit_large_lora(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer_LoRA(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        block_fn=partial(Block_LoRA, attn_class=MemEffAttention_LoRA),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "vit_giant2_lora",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def vit_giant2_lora(patch_size=16, num_register_tokens=0, **kwargs):\n    \"\"\"\n    Close to ViT-giant, with embed-dim 1536 and 24 heads => embed-dim per head 64\n    \"\"\"\n    model = DinoVisionTransformer_LoRA(\n        patch_size=patch_size,\n        embed_dim=1536,\n        depth=40,\n        num_heads=24,\n        mlp_ratio=4,",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "DINOv2_LoRA",
        "kind": 2,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "def DINOv2_LoRA(model_name, lora_r=0, lora_alpha=1, lora_dropout=0.0, lora_bias='none', **kwargs):\n    model_zoo_lora = {\n        \"vits\": vit_small_lora,\n        \"vitb\": vit_base_lora,\n        \"vitl\": vit_large_lora,\n        \"vitg\": vit_giant2_lora\n    }\n    if model_name not in model_zoo_lora:\n        raise ValueError(f\"Unknown DINOv2 LoRA model name: {model_name}\")\n    # Define base config and override with kwargs",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "depth_anything_v2.dinov2_lora",
        "description": "depth_anything_v2.dinov2_lora",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module",
        "detail": "depth_anything_v2.dinov2_lora",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature, out_feature, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_feature),\n            nn.ReLU(True)\n        )\n    def forward(self, x):\n        return self.conv_block(x)",
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(\n        self, \n        in_channels, \n        features=256, \n        use_bn=False, \n        out_channels=[256, 512, 1024, 1024], \n        use_clstoken=False\n    ):\n        super(DPTHead, self).__init__()",
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt",
        "description": "depth_anything_v2.dpt",
        "peekOfCode": "class DepthAnythingV2(nn.Module):\n    def __init__(\n        self, \n        encoder='vitl', \n        features=256, \n        out_channels=[256, 512, 1024, 1024], \n        use_bn=False, \n        use_clstoken=False,\n        max_depth=20.0\n    ):",
        "detail": "depth_anything_v2.dpt",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_features",
        "description": "depth_anything_v2.dpt_features",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature, out_feature, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_feature),\n            nn.ReLU(True)\n        )\n    def forward(self, x):\n        return self.conv_block(x)",
        "detail": "depth_anything_v2.dpt_features",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_features",
        "description": "depth_anything_v2.dpt_features",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(\n        self, \n        in_channels, \n        features=256, \n        use_bn=False, \n        out_channels=[256, 512, 1024, 1024], \n        use_clstoken=False\n    ):\n        super(DPTHead, self).__init__()",
        "detail": "depth_anything_v2.dpt_features",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_features",
        "description": "depth_anything_v2.dpt_features",
        "peekOfCode": "class DepthAnythingV2(nn.Module):\n    def __init__(\n        self, \n        encoder='vitl', \n        features=256, \n        out_channels=[256, 512, 1024, 1024], \n        use_bn=False, \n        use_clstoken=False,\n        max_depth=20.0\n    ):",
        "detail": "depth_anything_v2.dpt_features",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature,\n                      out_feature,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1), nn.BatchNorm2d(out_feature), nn.ReLU(True))\n    def forward(self, x):",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 features=256,\n                 use_bn=False,\n                 out_channels=[256, 512, 1024, 1024],\n                 use_clstoken=False):\n        super(DPTHead, self).__init__()\n        self.use_clstoken = use_clstoken\n        self.projects = nn.ModuleList([",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "class DepthAnythingV2(nn.Module):\n    def __init__(self,\n                 encoder='vitl',\n                 features=256,\n                 out_channels=[256, 512, 1024, 1024],\n                 use_bn=False,\n                 use_clstoken=False,\n                 max_depth=20.0):\n        super(DepthAnythingV2, self).__init__()\n        self.intermediate_layer_idx = {",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "get_interpolate_frames",
        "kind": 2,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "def get_interpolate_frames(frame_list_pre, frame_list_post):\n    # # Debugging: Check if frame_list_pre and frame_list_post are identical\n    # is_identical = True\n    # for idx in range(len(frame_list_pre)):\n    #     if not np.allclose(frame_list_pre[idx], frame_list_post[idx]):\n    #         is_identical = False\n    #         break\n    # print(f\"Are frame_list_pre and frame_list_post identical? {is_identical}\")\n    assert len(frame_list_pre) == len(frame_list_post)\n    min_w = 0.0",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "INFER_LEN",
        "kind": 5,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "INFER_LEN = 32\nOVERLAP = 10\nKEYFRAMES = [0, 12, 24, 25, 26, 27, 28, 29, 30, 31]\nINTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "OVERLAP",
        "kind": 5,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "OVERLAP = 10\nKEYFRAMES = [0, 12, 24, 25, 26, 27, 28, 29, 30, 31]\nINTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "KEYFRAMES",
        "kind": 5,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "KEYFRAMES = [0, 12, 24, 25, 26, 27, 28, 29, 30, 31]\nINTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "INTERP_LEN",
        "kind": 5,
        "importPath": "depth_anything_v2.dpt_live",
        "description": "depth_anything_v2.dpt_live",
        "peekOfCode": "INTERP_LEN = 8\ndef _make_fusion_block(features, use_bn, size=None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n        size=size,",
        "detail": "depth_anything_v2.dpt_live",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_feature, out_feature, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_feature),\n            nn.ReLU(True)\n        )\n    def forward(self, x):\n        return self.conv_block(x)",
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(\n        self, \n        in_channels, \n        features=256, \n        use_bn=False, \n        out_channels=[256, 512, 1024, 1024], \n        use_clstoken=False\n    ):\n        super(DPTHead, self).__init__()",
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "DepthAnythingV2_LoRA",
        "kind": 6,
        "importPath": "depth_anything_v2.dpt_lora",
        "description": "depth_anything_v2.dpt_lora",
        "peekOfCode": "class DepthAnythingV2_LoRA(nn.Module):\n    def __init__(\n        self, \n        encoder='vitl', \n        features=256, \n        out_channels=[256, 512, 1024, 1024], \n        use_bn=False, \n        use_clstoken=False,\n        max_depth=20.0,\n        # --- LoRA Args --- #",
        "detail": "depth_anything_v2.dpt_lora",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "kind": 2,
        "importPath": "util..ipynb_checkpoints.metric-checkpoint",
        "description": "util..ipynb_checkpoints.metric-checkpoint",
        "peekOfCode": "def eval_depth(pred, target):\n    assert pred.shape == target.shape\n    thresh = torch.max((target / pred), (pred / target))\n    d1 = torch.sum(thresh < 1.25).float() / len(thresh)\n    d2 = torch.sum(thresh < 1.25 ** 2).float() / len(thresh)\n    d3 = torch.sum(thresh < 1.25 ** 3).float() / len(thresh)\n    diff = pred - target\n    diff_log = torch.log(pred) - torch.log(target)\n    abs_rel = torch.mean(torch.abs(diff) / target)\n    sq_rel = torch.mean(torch.pow(diff, 2) / target)",
        "detail": "util..ipynb_checkpoints.metric-checkpoint",
        "documentation": {}
    },
    {
        "label": "setup_distributed",
        "kind": 2,
        "importPath": "util.dist_helper",
        "description": "util.dist_helper",
        "peekOfCode": "def setup_distributed(backend=\"nccl\", port=None):\n    \"\"\"AdaHessian Optimizer\n    Lifted from https://github.com/BIGBALLON/distribuuuu/blob/master/distribuuuu/utils.py\n    Originally licensed MIT, Copyright (c) 2020 Wei Li\n    \"\"\"\n    num_gpus = torch.cuda.device_count()\n    if \"SLURM_JOB_ID\" in os.environ:\n        rank = int(os.environ[\"SLURM_PROCID\"])\n        world_size = int(os.environ[\"SLURM_NTASKS\"])\n        node_list = os.environ[\"SLURM_NODELIST\"]",
        "detail": "util.dist_helper",
        "documentation": {}
    },
    {
        "label": "SiLogLoss",
        "kind": 6,
        "importPath": "util.loss",
        "description": "util.loss",
        "peekOfCode": "class SiLogLoss(nn.Module):\n    def __init__(self, lambd=0.5):\n        super().__init__()\n        self.lambd = lambd\n    def forward(self, pred, target, valid_mask):\n        valid_mask = valid_mask.detach()\n        diff_log = torch.log(target[valid_mask]) - torch.log(pred[valid_mask])\n        loss = torch.sqrt(torch.pow(diff_log, 2).mean() -\n                          self.lambd * torch.pow(diff_log.mean(), 2))\n        return loss",
        "detail": "util.loss",
        "documentation": {}
    },
    {
        "label": "eval_depth",
        "kind": 2,
        "importPath": "util.metric",
        "description": "util.metric",
        "peekOfCode": "def eval_depth(pred, target):\n    assert pred.shape == target.shape\n    thresh = torch.max((target / pred), (pred / target))\n    d1 = torch.sum(thresh < 1.25).float() / len(thresh)\n    d2 = torch.sum(thresh < 1.25 ** 2).float() / len(thresh)\n    d3 = torch.sum(thresh < 1.25 ** 3).float() / len(thresh)\n    diff = pred - target\n    diff_log = torch.log(pred) - torch.log(target)\n    abs_rel = torch.mean(torch.abs(diff) / target)\n    sq_rel = torch.mean(torch.pow(diff, 2) / target)",
        "detail": "util.metric",
        "documentation": {}
    },
    {
        "label": "init_log",
        "kind": 2,
        "importPath": "util.utils",
        "description": "util.utils",
        "peekOfCode": "def init_log(name, level=logging.INFO):\n    if (name, level) in logs:\n        return\n    logs.add((name, level))\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    if \"SLURM_PROCID\" in os.environ:\n        rank = int(os.environ[\"SLURM_PROCID\"])",
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "logs",
        "kind": 5,
        "importPath": "util.utils",
        "description": "util.utils",
        "peekOfCode": "logs = set()\ndef init_log(name, level=logging.INFO):\n    if (name, level) in logs:\n        return\n    logs.add((name, level))\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    if \"SLURM_PROCID\" in os.environ:",
        "detail": "util.utils",
        "documentation": {}
    },
    {
        "label": "print_table",
        "kind": 2,
        "importPath": "check_model_weights",
        "description": "check_model_weights",
        "peekOfCode": "def print_table(headers, data):\n    \"\"\"\n    n    Args:\n        headers (list): n        data (list): n    \"\"\"\n    if not data:\n        print(\"")\n        return",
        "detail": "check_model_weights",
        "documentation": {}
    },
    {
        "label": "get_model_weights_info",
        "kind": 2,
        "importPath": "check_model_weights",
        "description": "check_model_weights",
        "peekOfCode": "def get_model_weights_info(model: nn.Module):\n    \"\"\"\n     state_dict n    Args:\n        model (nn.Module): PyTorch n    Returns:\n        list: \n              (, , ?. n    \"\"\"\n    weights_info = []",
        "detail": "check_model_weights",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "depth_to_pointcloud",
        "description": "depth_to_pointcloud",
        "peekOfCode": "def main():\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description='Generate depth maps and point clouds from images.')\n    parser.add_argument('--encoder', default='vitl', type=str, choices=['vits', 'vitb', 'vitl', 'vitg'],\n                        help='Model encoder to use.')\n    parser.add_argument('--load-from', default='', type=str, required=True,\n                        help='Path to the pre-trained model weights.')\n    parser.add_argument('--max-depth', default=20, type=float,\n                        help='Maximum depth value for the depth map.')\n    parser.add_argument('--img-path', type=str, required=True,",
        "detail": "depth_to_pointcloud",
        "documentation": {}
    },
    {
        "label": "depth_to_colormap",
        "kind": 2,
        "importPath": "evaluate",
        "description": "evaluate",
        "peekOfCode": "def depth_to_colormap(depth_map: np.ndarray) -> np.ndarray:\n    \"\"\"\n    n    Args:\n        depth_map (np.ndarray): n    Returns:\n        np.ndarray: RGB (0-255)n    \"\"\"\n    # ?-1\n    min_val = np.min(depth_map)",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "evaluate",
        "description": "evaluate",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace(\"module.\", \"\")\n        new_state_dict[new_key] = v\n    return new_state_dict\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "normalize_and_save_image",
        "kind": 2,
        "importPath": "evaluate",
        "description": "evaluate",
        "peekOfCode": "def normalize_and_save_image(data: np.ndarray, path: str, is_dino_feature: bool = False):\n    \"\"\"\n    0-255PNGn    Args:\n        data (np.ndarray): DINOn        path (str): n        is_dino_feature (bool): INOn    \"\"\"\n    if is_dino_feature and data.ndim == 3: # C, H, W\n        # For DINO features, take the first channel for visualization or average",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evaluate",
        "description": "evaluate",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\"{}\\n\".format(pprint.pformat(all_args)))\n    writer = SummaryWriter(args.save_path)\n    # valset = C3VD('/root/Depth-Anything-V2/metric_depth/dataset/splits/c3vd/val.txt', 'val', size=size, max_depth=args.max_depth)\n    valset = ConcatDataset([\n        # C3VD(",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "evaluate",
        "description": "evaluate",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\"--save-path\", type=str, required=True)\nparser.add_argument(\"--load-from\",\n                    type=str,\n                    required=True,\n                    help=\"Path to the model checkpoint\")",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": "evaluate",
        "description": "evaluate",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\ndef main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\"{}\\n\".format(pprint.pformat(all_args)))\n    writer = SummaryWriter(args.save_path)\n    # valset = C3VD('/root/Depth-Anything-V2/metric_depth/dataset/splits/c3vd/val.txt', 'val', size=size, max_depth=args.max_depth)\n    valset = ConcatDataset([",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "evaluate_lora",
        "description": "evaluate_lora",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n        new_state_dict[new_key] = v\n    return new_state_dict\ndef verify_model_weights_loaded(model: torch.nn.Module,\n                                processed_state_dict_to_load: OrderedDict,\n                                logger: logging.Logger = None):\n    if logger is None:",
        "detail": "evaluate_lora",
        "documentation": {}
    },
    {
        "label": "verify_model_weights_loaded",
        "kind": 2,
        "importPath": "evaluate_lora",
        "description": "evaluate_lora",
        "peekOfCode": "def verify_model_weights_loaded(model: torch.nn.Module,\n                                processed_state_dict_to_load: OrderedDict,\n                                logger: logging.Logger = None):\n    if logger is None:\n        log_fn = print\n    else:\n        log_fn = logger.info\n    model_state_dict = model.state_dict()  # n    model_keys = set(model_state_dict.keys())\n    loaded_keys = set(processed_state_dict_to_load.keys())",
        "detail": "evaluate_lora",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evaluate_lora",
        "description": "evaluate_lora",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # Determine project root\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    project_root = os.path.abspath(os.path.join(script_dir, \"..\"))\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    if args.save_path:\n        os.makedirs(args.save_path, exist_ok=True)\n    logger.info(\"\\n{}\\n\".format(pprint.pformat(vars(args))))",
        "detail": "evaluate_lora",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "evaluate_lora",
        "description": "evaluate_lora",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 LoRA Evaluation for Metric Depth Estimation\"\n)\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[",
        "detail": "evaluate_lora",
        "documentation": {}
    },
    {
        "label": "merge_state_dicts",
        "kind": 2,
        "importPath": "evaluate_lora_old",
        "description": "evaluate_lora_old",
        "peekOfCode": "def merge_state_dicts(lora_weights, backbone_weights, logger):\n    \"\"\"\n    ackboneora\n    Args:\n        lora_weights (dict): LoRA\n        backbone_weights (dict): n        logger (logging.Logger): n    Returns:\n        dict: n    \"\"\"",
        "detail": "evaluate_lora_old",
        "documentation": {}
    },
    {
        "label": "load_weights_from_checkpoints",
        "kind": 2,
        "importPath": "evaluate_lora_old",
        "description": "evaluate_lora_old",
        "peekOfCode": "def load_weights_from_checkpoints(model, checkpoint_path, dino_weights_path, args, script_dir, logger):\n    \"\"\"\n    n    Args:\n        model: \n        checkpoint_path (str): LoRA\n        dino_weights_path (str): DINOn        args: n        script_dir (str): n        logger (logging.Logger): ?,
        "detail": "evaluate_lora_old",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evaluate_lora_old",
        "description": "evaluate_lora_old",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # Determine project root\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    project_root = os.path.abspath(os.path.join(script_dir, \"..\"))\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    if args.save_path:\n        os.makedirs(args.save_path, exist_ok=True)\n    logger.info(\"\\n{}\\n\".format(pprint.pformat(vars(args))))",
        "detail": "evaluate_lora_old",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "evaluate_lora_old",
        "description": "evaluate_lora_old",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 LoRA Evaluation for Metric Depth Estimation\"\n)\nparser.add_argument(\n    \"--encoder\", default=\"vitl\", choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"]\n)\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"kitti\", \"c3vd\", \"simcol\", \"endomapper\", \"combined\"]",
        "detail": "evaluate_lora_old",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evaluate_performance",
        "description": "evaluate_performance",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\n        \"Performance Test Configuration:\\n{}\\n\".format(pprint.pformat(all_args))\n    )\n    # --- Model Loading ---\n    model_configs = {",
        "detail": "evaluate_performance",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "evaluate_performance",
        "description": "evaluate_performance",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 Metric Depth Performance Evaluation\"\n)\nparser.add_argument(\n    \"--encoder\", default=\"vitl\", choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"]\n)\nparser.add_argument(\"--img-size\", default=518, type=int)\nparser.add_argument(\n    \"--load-from\", type=str, required=True, help=\"Path to the model checkpoint\"\n)",
        "detail": "evaluate_performance",
        "documentation": {}
    },
    {
        "label": "preprocess_frame",
        "kind": 2,
        "importPath": "evaluate_performance_with_io",
        "description": "evaluate_performance_with_io",
        "peekOfCode": "def preprocess_frame(frame, target_size):\n    \"\"\"Converts OpenCV frame to model input tensor.\"\"\"\n    if frame is None:\n        return None\n    # 1. BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # 2. To Tensor (HWC -> CHW) and Normalize to [0, 1]\n    tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0\n    # 3. Resize\n    # Use align_corners=True consistent with many DPT models if needed, else False might be better",
        "detail": "evaluate_performance_with_io",
        "documentation": {}
    },
    {
        "label": "postprocess_depth",
        "kind": 2,
        "importPath": "evaluate_performance_with_io",
        "description": "evaluate_performance_with_io",
        "peekOfCode": "def postprocess_depth(pred_depth, original_hw):\n    \"\"\"Converts raw model output to a displayable depth map.\"\"\"\n    # 1. Resize to original frame size\n    # Use align_corners=True matching potential use in model, else False\n    resized_pred = F.interpolate(\n        pred_depth.unsqueeze(0).unsqueeze(0),\n        size=original_hw,\n        mode=\"bicubic\",\n        align_corners=False,\n    ).squeeze()",
        "detail": "evaluate_performance_with_io",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evaluate_performance_with_io",
        "description": "evaluate_performance_with_io",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    all_args = {**vars(args)}\n    logger.info(\n        \"Real-time Inference Configuration:\\n{}\\n\".format(pprint.pformat(all_args))\n    )\n    # --- Model Loading ---\n    model_configs = {",
        "detail": "evaluate_performance_with_io",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "evaluate_performance_with_io",
        "description": "evaluate_performance_with_io",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 Real-time Inference from Camera\"\n)\n# --- Model Arguments ---\nparser.add_argument(\n    \"--encoder\", default=\"vitl\", choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"]\n)\nparser.add_argument(\n    \"--load-from\", type=str, required=True, help=\"Path to the model checkpoint\"\n)",
        "detail": "evaluate_performance_with_io",
        "documentation": {}
    },
    {
        "label": "MEAN",
        "kind": 5,
        "importPath": "evaluate_performance_with_io",
        "description": "evaluate_performance_with_io",
        "peekOfCode": "MEAN = torch.tensor([0.485, 0.456, 0.406]).cuda().view(3, 1, 1)\nSTD = torch.tensor([0.229, 0.224, 0.225]).cuda().view(3, 1, 1)\ndef preprocess_frame(frame, target_size):\n    \"\"\"Converts OpenCV frame to model input tensor.\"\"\"\n    if frame is None:\n        return None\n    # 1. BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # 2. To Tensor (HWC -> CHW) and Normalize to [0, 1]\n    tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0",
        "detail": "evaluate_performance_with_io",
        "documentation": {}
    },
    {
        "label": "STD",
        "kind": 5,
        "importPath": "evaluate_performance_with_io",
        "description": "evaluate_performance_with_io",
        "peekOfCode": "STD = torch.tensor([0.229, 0.224, 0.225]).cuda().view(3, 1, 1)\ndef preprocess_frame(frame, target_size):\n    \"\"\"Converts OpenCV frame to model input tensor.\"\"\"\n    if frame is None:\n        return None\n    # 1. BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # 2. To Tensor (HWC -> CHW) and Normalize to [0, 1]\n    tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0\n    # 3. Resize",
        "detail": "evaluate_performance_with_io",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "run-Copy1",
        "description": "run-Copy1",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace('module.', '') if k.startswith('module.') else k\n        new_state_dict[new_key] = v\n    return new_state_dict\n# python run.py --img-path /root/c3vd/test_mapping.txt --input-size 518 --outdir ./vis_depth --encoder vitl --load-from /root/Depth-Anything-V2/checkpoints/tmp/50.pth --grayscale\n# python run.py --img-path /data/c3vd/test/color/trans_t4_a/0381_color.png --input-size 518 --outdir ./vis_depth --encoder vits --load-from /data/train_combined/best_abs_rel.pth --max-depth 0.2 --grayscale\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Depth Anything V2 Metric Depth Estimation')",
        "detail": "run-Copy1",
        "documentation": {}
    },
    {
        "label": "TemporalSmoother",
        "kind": 6,
        "importPath": "run-Copy2",
        "description": "run-Copy2",
        "peekOfCode": "class TemporalSmoother:\n    \"\"\"\n    Manages temporal smoothing of depth maps using a sliding window.\n    \"\"\"\n    def __init__(self,\n                 window_size: int = 30,\n                 sigma: float = 0.01,\n                 init_method: str = 'dynamic',\n                 device: str = 'cpu'):\n        self.N = window_size",
        "detail": "run-Copy2",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "run-Copy2",
        "description": "run-Copy2",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace('module.', '') if k.startswith('module.') else k\n        new_state_dict[new_key] = v\n    return new_state_dict\nclass TemporalSmoother:\n    \"\"\"\n    Manages temporal smoothing of depth maps using a sliding window.\n    \"\"\"",
        "detail": "run-Copy2",
        "documentation": {}
    },
    {
        "label": "TemporalSmoother",
        "kind": 6,
        "importPath": "run",
        "description": "run",
        "peekOfCode": "class TemporalSmoother:\n    \"\"\"\n    Manages temporal smoothing of depth maps using multiple sliding windows,\n    one for each combination of specified initialization and scaling methods.\n    \"\"\"\n    def __init__(self,\n                 window_size: int = 30,\n                 sigma: float = 0.01,\n                 init_methods: list[str] = ['dynamic'],\n                 scale_methods: list[str] = ['least_squares'],",
        "detail": "run",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "run",
        "description": "run",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    \"\"\"Removes the 'module.' prefix from state dictionary keys.\"\"\"\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace('module.', '') if k.startswith('module.') else k\n        new_state_dict[new_key] = v\n    return new_state_dict\nclass TemporalSmoother:\n    \"\"\"\n    Manages temporal smoothing of depth maps using multiple sliding windows,",
        "detail": "run",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "run2",
        "description": "run2",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace('module.', '') if k.startswith('module.') else k\n        new_state_dict[new_key] = v\n    return new_state_dict\ndef visualize_depth(img, pred_depth, output_path):\n    \"\"\"\n    n    Args:",
        "detail": "run2",
        "documentation": {}
    },
    {
        "label": "visualize_depth",
        "kind": 2,
        "importPath": "run2",
        "description": "run2",
        "peekOfCode": "def visualize_depth(img, pred_depth, output_path):\n    \"\"\"\n    n    Args:\n        img (torch.Tensor): \n        pred_depth (torch.Tensor): n        output_path (str): \n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np",
        "detail": "run2",
        "documentation": {}
    },
    {
        "label": "save_video",
        "kind": 2,
        "importPath": "run_live",
        "description": "run_live",
        "peekOfCode": "def save_video(frames,\n               output_video_path,\n               fps=10,\n               is_depths=False,\n               grayscale=False):\n    writer = imageio.get_writer(output_video_path,\n                                fps=fps,\n                                macro_block_size=1,\n                                codec='libx264',\n                                ffmpeg_params=['-crf', '18'])",
        "detail": "run_live",
        "documentation": {}
    },
    {
        "label": "strip_module_prefix",
        "kind": 2,
        "importPath": "run_live",
        "description": "run_live",
        "peekOfCode": "def strip_module_prefix(state_dict):\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        new_key = k.replace(\"module.\", \"\")\n        new_state_dict[new_key] = v\n    return new_state_dict\ndef read_image_frames_from_txt(\n        filelist_path: str, image_width: int,\n        image_height: int) -> Tuple[List[np.ndarray], List[str], int]:\n    \"\"\"",
        "detail": "run_live",
        "documentation": {}
    },
    {
        "label": "read_image_frames_from_txt",
        "kind": 2,
        "importPath": "run_live",
        "description": "run_live",
        "peekOfCode": "def read_image_frames_from_txt(\n        filelist_path: str, image_width: int,\n        image_height: int) -> Tuple[List[np.ndarray], List[str], int]:\n    \"\"\"\n    Reads image paths from a text file, loads images, resizes them, and converts to numpy frames.\n    Ignores depth and mask information. Returns frames, original image paths, and a dummy fps.\n    \"\"\"\n    frames: List[np.ndarray] = []\n    image_paths: List[str] = []\n    with open(filelist_path, \"r\") as f:",
        "detail": "run_live",
        "documentation": {}
    },
    {
        "label": "GradientLoss",
        "kind": 6,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "class GradientLoss(nn.Module):\n    \"\"\"GradientLoss.\n    Adapted from https://www.cs.cornell.edu/projects/megadepth/\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n    \"\"\"\n    def __init__(\n        self, valid_mask=True, loss_weight=1.0, max_depth=None, loss_name=\"loss_grad\"",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "SigLoss",
        "kind": 6,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "class SigLoss(nn.Module):\n    \"\"\"SigLoss.\n        This follows `AdaBins <https://arxiv.org/abs/2011.14141>`_.\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n        warm_up (bool): A simple warm up stage to help convergence. Default: False.\n        warm_iter (int): The number of warm up stage. Default: 100.\n    \"\"\"",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "decode_head_linear",
        "kind": 6,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "class decode_head_linear(torch.nn.Module):\n    def __init__(\n        self,\n        input_transform=\"resize_concat\",\n        image_shape=(224, 224),\n        in_index=(0, 1, 2, 3),\n        upsample=4,\n        min_depth=0.001,\n        max_depth=150,\n        classify=True,",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "Dinov2ForDepth",
        "kind": 6,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "class Dinov2ForDepth(torch.nn.Module):\n    def __init__(\n        self, backbone_size=\"base\", image_shape=(224, 224), decode_type=\"linear4\"\n    ):\n        super().__init__()\n        self.backbone_size = backbone_size\n        self.backbone_archs = {\n            \"small\": \"vits14\",\n            \"base\": \"vitb14\",\n            \"large\": \"vitl14\",",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "_LoRA_qkv",
        "kind": 6,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "class _LoRA_qkv(nn.Module):\n    \"\"\"In Dinov2 it is implemented as\n    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n    B, N, C = x.shape\n    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n    q, k, v = qkv.unbind(0)\n    \"\"\"\n    def __init__(\n        self,\n        qkv: nn.Module,",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "SurgicalDINO",
        "kind": 6,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "class SurgicalDINO(nn.Module):\n    \"\"\"Applies low-rank adaptation to Dinov2 model's image encoder.\n    Args:\n        backbone_size: the pretrained size of dinov2 model\n        r: rank of LoRA\n        image_shape: input image shape\n        decode_type: the decode type of decode head, \"linear\" or \"\"\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "surgicaldino",
        "description": "surgicaldino",
        "peekOfCode": "def resize(\n    input,\n    size=None,\n    scale_factor=None,\n    mode=\"nearest\",\n    align_corners=None,\n    warning=False,\n):\n    if warning:\n        if size is not None and align_corners:",
        "detail": "surgicaldino",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": "train-Copy1",
        "description": "train-Copy1",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item['image'].shape[-2])\n        max_w = max(max_w, item['image'].shape[-1])\n    # ?14\n    #  Resize  ensure_multiple_of\n    # ?Resize ?,
        "detail": "train-Copy1",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train-Copy1",
        "description": "train-Copy1",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:\n        all_args = {**vars(args), \"ngpus\": world_size}\n        logger.info(\"{}\\n\".format(pprint.pformat(all_args)))\n        writer = SummaryWriter(args.save_path)",
        "detail": "train-Copy1",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "train-Copy1",
        "description": "train-Copy1",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\"\n)\nparser.add_argument(\n    \"--encoder\", default=\"vitl\", choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"]\n)\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],",
        "detail": "train-Copy1",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item['image'].shape[-2])\n        max_w = max(max_w, item['image'].shape[-1])\n    # ?14\n    #  Resize  ensure_multiple_of\n    # ?Resize ?,
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:\n        all_args = {**vars(args), \"ngpus\": world_size}\n        logger.info(\"{}\\n\".format(pprint.pformat(all_args)))\n        writer = SummaryWriter(args.save_path)",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "C3VD_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "C3VD_TRAIN_SPLIT = \"/data/c3vd/train.txt\"\nC3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "C3VD_VAL_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "C3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "ENDOMAPER_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "ENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "INHOUSE_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "INHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "SIMCOL_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "SIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "SIMCOL_VAL_SPLIT",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "SIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nWORKS=16\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "train-Copy2",
        "description": "train-Copy2",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],\n)",
        "detail": "train-Copy2",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item['image'].shape[-2])\n        max_w = max(max_w, item['image'].shape[-1])\n    stride = 14\n    if max_h % stride != 0:\n        max_h = max_h + (stride - max_h % stride)",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # PyTorch 2.0+ TF32AmpereGPUn    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "C3VD_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "C3VD_TRAIN_SPLIT = \"/data/c3vd/train.txt\"\nC3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "C3VD_VAL_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "C3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "ENDOMAPER_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "ENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "INHOUSE_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "INHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "SIMCOL_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "SIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "SIMCOL_VAL_SPLIT",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "SIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "train_4_diff",
        "description": "train_4_diff",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],\n)",
        "detail": "train_4_diff",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item['image'].shape[-2])\n        max_w = max(max_w, item['image'].shape[-1])\n    stride = 14\n    if max_h % stride != 0:\n        max_h = max_h + (stride - max_h % stride)",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # PyTorch 2.0+ TF32AmpereGPUn    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "C3VD_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "C3VD_TRAIN_SPLIT = \"/data/c3vd/train.txt\"\nC3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "C3VD_VAL_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "C3VD_VAL_SPLIT = \"/data/c3vd/test.txt\"\nENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "ENDOMAPER_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "ENDOMAPER_TRAIN_SPLIT = \"/data/endomapper_sim/file_list.txt\"\nINHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "INHOUSE_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "INHOUSE_TRAIN_SPLIT = \"/data/inhouse/train.txt\"\nINHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "INHOUSE_VAL_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "INHOUSE_VAL_SPLIT = \"/data/inhouse/val.txt\"\nSIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "SIMCOL_TRAIN_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "SIMCOL_TRAIN_SPLIT = \"/data/simcol/train_paths.txt\"\nSIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "SIMCOL_VAL_SPLIT",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "SIMCOL_VAL_SPLIT = \"/data/simcol/val_paths.txt\"\nparser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "train_4_same",
        "description": "train_4_same",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\"],\n)",
        "detail": "train_4_same",
        "documentation": {}
    },
    {
        "label": "collate_fn_pad",
        "kind": 2,
        "importPath": "train_lora",
        "description": "train_lora",
        "peekOfCode": "def collate_fn_pad(batch):\n    # n    max_h = 0\n    max_w = 0\n    for item in batch:\n        max_h = max(max_h, item[\"image\"].shape[-2])\n        max_w = max(max_w, item[\"image\"].shape[-1])\n    # ?14\n    #  Resize  ensure_multiple_of\n    # ?Resize ?,
        "detail": "train_lora",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train_lora",
        "description": "train_lora",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    # Determine project root based on script location\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    project_root = os.path.abspath(os.path.join(script_dir, \"..\"))\n    warnings.simplefilter(\"ignore\", np.RankWarning)\n    logger = init_log(\"global\", logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:",
        "detail": "train_lora",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "train_lora",
        "description": "train_lora",
        "peekOfCode": "parser = argparse.ArgumentParser(\n    description=\"Depth Anything V2 LoRA Training for Metric Depth Estimation\")\nparser.add_argument(\"--encoder\",\n                    default=\"vitl\",\n                    choices=[\"vits\", \"vitb\", \"vitl\", \"vitg\"])\nparser.add_argument(\n    \"--dataset\",\n    default=\"hypersim\",\n    choices=[\"hypersim\", \"vkitti\", \"c3vd\", \"simcol\", \"combined\", \"endomapper\"],\n)",
        "detail": "train_lora",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train_single",
        "description": "train_single",
        "peekOfCode": "def main():\n    args = parser.parse_args()\n    warnings.simplefilter('ignore', np.RankWarning)\n    logger = init_log('global', logging.INFO)\n    logger.propagate = 0\n    rank, world_size = setup_distributed(port=args.port)\n    if rank == 0:\n        all_args = {**vars(args), 'ngpus': world_size}\n        logger.info('{}\\n'.format(pprint.pformat(all_args)))\n        writer = SummaryWriter(args.save_path)",
        "detail": "train_single",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "train_single",
        "description": "train_single",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Depth Anything V2 for Metric Depth Estimation')\nparser.add_argument('--encoder', default='vitl', choices=['vits', 'vitb', 'vitl', 'vitg'])\nparser.add_argument('--dataset', default='hypersim', choices=['hypersim', 'vkitti'])\nparser.add_argument('--img-size', default=518, type=int)\nparser.add_argument('--min-depth', default=0.001, type=float)\nparser.add_argument('--max-depth', default=20, type=float)\nparser.add_argument('--epochs', default=40, type=int)\nparser.add_argument('--bs', default=2, type=int)\nparser.add_argument('--lr', default=0.000005, type=float)\nparser.add_argument('--pretrained-from', type=str)",
        "detail": "train_single",
        "documentation": {}
    },
    {
        "label": "print_keys_with_depth",
        "kind": 2,
        "importPath": "weight",
        "description": "weight",
        "peekOfCode": "def print_keys_with_depth(data, current_depth, max_depth):\n    \"\"\"\n    Recursively prints keys of nested dictionaries or elements up to a specified depth.\n    Args:\n        data (dict or any): The data structure to traverse.\n        current_depth (int): The current depth of the traversal.\n        max_depth (int): The maximum depth to print keys.\n    \"\"\"\n    if current_depth > max_depth:\n        return",
        "detail": "weight",
        "documentation": {}
    }
]